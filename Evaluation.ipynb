{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string \n",
    "import pickle\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix as sparse_matrix\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize  \n",
    "\n",
    "from gensim.test import utils\n",
    "from gensim.models import KeyedVectors, nmf\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_talks = pd.read_csv('../Data/talks_data.csv')\n",
    "df_users = pd.read_csv('../users_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_talks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../Data/TED_train.csv')\n",
    "df_test  = pd.read_csv('../Data/TED_test.csv')\n",
    "talks_ids = {k:i for i,k in pickle.load(open('../Data/dict_talks_idx.pickle', 'rb')).items()}\n",
    "users_ids = {k:i for i,k in pickle.load(open('../Data/dict_users_idx.pickle', 'rb')).items()}\n",
    "\n",
    "df_train['user'] = df_train['user_id'].apply(lambda u: users_ids[u])\n",
    "df_train['talk'] = df_train['talk_id'].apply(lambda u: talks_ids[u])\n",
    "df_test['user'] = df_test['user_id'].apply(lambda u: users_ids[u])\n",
    "df_test['talk'] = df_test['talk_id'].apply(lambda u: talks_ids[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [d for d in os.listdir('/data/ted_kg_embeddings') if not d.endswith('.sh')]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = { 'transd':{} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rep in representations:\n",
    "    representations[rep] = {}\n",
    "    for dataset in tqdm(datasets):\n",
    "        try:\n",
    "            if os.path.exists('/data/ted_kg_embeddings/'+dataset+'/embeddings/'+rep+'/ent_embedding.tsv'):\n",
    "                df = pd.read_csv('/data/ted_kg_embeddings/'+dataset+'/embeddings/'+rep+'/ent_embedding.tsv', sep='\\t', header=None)\n",
    "                df_labels = pd.read_csv('/data/ted_kg_embeddings/'+dataset+'/embeddings/'+rep+'/ent_labels.tsv', sep='\\t', header=None)\n",
    "\n",
    "                df_data = pd.DataFrame(np.hstack([df_labels.values, df.values]))\n",
    "\n",
    "                transe_representations = {v[0]: v[1:] for v in df_data.values}\n",
    "                embeddings = []\n",
    "\n",
    "                for talk_hash in df_talks.id.values:\n",
    "                    embeddings.append(transe_representations[talk_hash])\n",
    "\n",
    "                representations[rep][dataset] = np.array(embeddings)\n",
    "                print(rep, representations[rep][dataset].shape)\n",
    "            else:\n",
    "                print('Embeddings not computed yet for', dataset)\n",
    "        except Exception as e:\n",
    "            print('Problem with', rep, '-', dataset, ':', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(representations['transd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk2idx = {v:i for i,v in enumerate(df_talks.id)}\n",
    "idx2talk = {i:v for v,i in talk2idx.items()}\n",
    "title2idx = {v:i for i,v in enumerate(df_talks.title)}\n",
    "idx2title = {i:v for v,i in title2idx.items()}\n",
    "title2hash = {t:h for t,h in df_talks[['title', 'id']].values}\n",
    "hash2title = {i:v for v,i in title2hash.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks_likes = {}\n",
    "less_than_two = []\n",
    "for talk_idx, e in df_train.groupby('talk'):\n",
    "    talks_likes[talk_idx] = e['user'].values.tolist()\n",
    "    if len(e['user'].values.tolist()) < 2:\n",
    "        less_than_two.append(talk_idx)\n",
    "len([l for l in talks_likes if len(talks_likes[l]) != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for talk_idx, e in df_test.groupby('talk'):\n",
    "    if talk_idx in talks_likes:\n",
    "        talks_likes[talk_idx].extend(e['user'].values.tolist())\n",
    "    else:\n",
    "        talks_likes[talk_idx] = e['user'].values.tolist()\n",
    "\n",
    "    if len(talks_likes[talk_idx]) < 2:\n",
    "        less_than_two.append(talk_idx)\n",
    "len([l for l in talks_likes if len(talks_likes[l]) != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_talks = {}\n",
    "for t, rv in df_talks[['title', 'related_videos']].values:\n",
    "    try:\n",
    "        related_talks[t] = rv.split(';')\n",
    "    except:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2 Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "\n",
    "for rep in representations:\n",
    "    predictions[rep] = {}\n",
    "    for dataset in tqdm(representations[rep]):\n",
    "        talk_features = representations[rep][dataset]\n",
    "        sim_matrix = cosine_similarity(talk_features, talk_features)\n",
    "        predictions[rep][dataset] = {idx: sim_matrix[idx].argsort()[::-1]\n",
    "                                        for idx in range(len(sim_matrix))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "K = 10\n",
    "\n",
    "for rep in tqdm(representations):\n",
    "    metrics[rep] = {}\n",
    "    for dataset in tqdm(representations[rep]):\n",
    "        metrics[rep][dataset] = {'hitrate@10': [], 'mmr@10': [], 'ndcg@10': []}\n",
    "\n",
    "        for talk_idx in range((len(sim_matrix))):\n",
    "            talk_title = idx2title[talk_idx]\n",
    "            if talk_title not in related_talks:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                user_recs = predictions[rep][dataset][talk_idx][1:] # Item 0 is the talk itself, sim == 1\n",
    "                norm_hitrate = sum(1 for _ in related_talks[talk_title])\n",
    "                norm_mmr     = sum(1/(r+1) for r, _ in enumerate(related_talks[talk_title]))\n",
    "                norm_ndcg    = sum(math.log(2)/math.log(r+2) for r, _ in enumerate(related_talks[talk_title]))\n",
    "\n",
    "                hitrate = 0\n",
    "                mmr = 0\n",
    "                ndcg = 0\n",
    "\n",
    "                for rec_title in related_talks[talk_title]:\n",
    "                    rec_idx = title2idx[rec_title]\n",
    "                    rec_rank = np.where(user_recs==rec_idx)[0][0] + 1\n",
    "                    # print(user_recs)\n",
    "\n",
    "                    hitrate += 0 if rec_rank > K else 1\n",
    "                    mmr     += 0 if rec_rank > K else 1/rec_rank\n",
    "                    ndcg    += 0 if rec_rank > K else math.log(2)/math.log(1+rec_rank)\n",
    "\n",
    "                metrics[rep][dataset]['hitrate@10'].append(hitrate/norm_hitrate)\n",
    "                metrics[rep][dataset]['mmr@10'].append(mmr/norm_mmr)\n",
    "                metrics[rep][dataset]['ndcg@10'].append(ndcg/norm_ndcg)\n",
    "            except Exception as e:\n",
    "                print(talk_idx, rep, str(e))\n",
    "\n",
    "        metrics[rep][dataset]['hitrate@10'] = np.mean(metrics[rep][dataset]['hitrate@10'])\n",
    "        metrics[rep][dataset]['mmr@10'] = np.mean(metrics[rep][dataset]['mmr@10'])\n",
    "        metrics[rep][dataset]['ndcg@10'] = np.mean(metrics[rep][dataset]['ndcg@10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rep in metrics:\n",
    "    for dataset in metrics[rep]:\n",
    "        print(rep.upper(), '-', dataset)\n",
    "        for metric in metrics[rep][dataset]:\n",
    "            print('    ' + metric.upper() + ':', round(metrics[rep][dataset][metric], 4))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"combo_sim = []\n",
    "for rep in ['transe', 'transd', 'transh', 'transm', 'transr']:\n",
    "    talk_features = representations[rep]\n",
    "    if len(combo_sim) == 0:\n",
    "        combo_sim = cosine_similarity(talk_features, talk_features)\n",
    "    else:\n",
    "        combo_sim += cosine_similarity(talk_features, talk_features)\n",
    "\n",
    "combo_1 = {idx: combo_sim[idx].argsort()[::-1] for idx in range(len(sim_matrix))}\n",
    "\n",
    "rep = 'combo_1'\n",
    "metrics[rep] = {'hitrate@10': [], 'mmr@10': [], 'ndcg@10': []}\n",
    "\n",
    "for talk_idx in range((len(sim_matrix))):\n",
    "    talk_title = idx2title[talk_idx]\n",
    "    if talk_title not in related_talks:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        user_recs = combo_1[talk_idx][1:] # Item 0 is the talk itself, sim == 1\n",
    "        norm_hitrate = sum(1 for _ in related_talks[talk_title])\n",
    "        norm_mmr     = sum(1/(r+1) for r, _ in enumerate(related_talks[talk_title]))\n",
    "        norm_ndcg    = sum(math.log(2)/math.log(r+2) for r, _ in enumerate(related_talks[talk_title]))\n",
    "\n",
    "        hitrate = 0\n",
    "        mmr = 0\n",
    "        ndcg = 0\n",
    "\n",
    "        for rec_title in related_talks[talk_title]:\n",
    "            rec_idx = title2idx[rec_title]\n",
    "            rec_rank = np.where(user_recs==rec_idx)[0][0] + 1\n",
    "            # print(user_recs)\n",
    "\n",
    "            hitrate += 0 if rec_rank > K else 1\n",
    "            mmr     += 0 if rec_rank > K else 1/rec_rank\n",
    "            ndcg    += 0 if rec_rank > K else math.log(2)/math.log(1+rec_rank)\n",
    "\n",
    "        metrics[rep]['hitrate@10'].append(hitrate/norm_hitrate)\n",
    "        metrics[rep]['mmr@10'].append(mmr/norm_mmr)\n",
    "        metrics[rep]['ndcg@10'].append(ndcg/norm_ndcg)\n",
    "    except:\n",
    "        print(talk_idx)\n",
    "\n",
    "metrics[rep]['hitrate@10'] = np.mean(metrics[rep]['hitrate@10'])\n",
    "metrics[rep]['mmr@10'] = np.mean(metrics[rep]['mmr@10'])\n",
    "metrics[rep]['ndcg@10'] = np.mean(metrics[rep]['ndcg@10'])\n",
    "\n",
    "print(rep.upper())\n",
    "for metric in metrics[rep]:\n",
    "    print('    ' + metric.upper() + ':', round(metrics[rep][metric], 4))\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1 Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {}\n",
    "test_dataset = {}\n",
    "\n",
    "for user, e in tqdm(df_train.groupby('user')):\n",
    "    train_dataset[user] = e['talk'].values.tolist()\n",
    "\n",
    "for user, e in tqdm(df_test.groupby('user')):\n",
    "    test_dataset[user] = e['talk'].values.tolist()\n",
    "    \n",
    "\n",
    "user2idx = {u:i for i,u in enumerate(sorted(test_dataset.keys()))}\n",
    "idx2user = {i:u for u,i in user2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations['transd']['metadata_ner_min10_none_none'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectors = {}\n",
    "for rep in representations:\n",
    "    user_vectors[rep] = {}\n",
    "    for dataset in representations[rep]:\n",
    "            try:\n",
    "                features = representations[rep][dataset]\n",
    "                user_vectors[rep][dataset] = []\n",
    "                for user in tqdm(sorted(user2idx.keys()), desc=dataset + ' - ' + rep):\n",
    "                    avg_features = np.mean([features[talk2idx[talk]] for talk in train_dataset[user]], axis=0)\n",
    "                    user_vectors[rep][dataset].append(avg_features)\n",
    "            except Exception as e:\n",
    "                print('Problem with', rep, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = {}\n",
    "for rep in representations:\n",
    "    similarities[rep] = {}\n",
    "    for dataset in tqdm(representations[rep]):\n",
    "        simmat = cosine_similarity(user_vectors[rep][dataset], representations[rep][dataset])\n",
    "        similarities[rep][dataset] = {user: simmat[user2idx[user]].argsort()[::-1] for user in test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_1 = {}\n",
    "for rep in representations:\n",
    "    metrics_1[rep] = {}\n",
    "    for dataset in tqdm(representations[rep]):\n",
    "        metrics_1[rep][dataset] = {'hitrate@10': [], 'mmr@10': [], 'ndcg@10': []}\n",
    "        for user in test_dataset:\n",
    "            already_seen = [talk2idx[t] for t in train_dataset[user]]\n",
    "            one_out      = talk2idx[test_dataset[user][0]]\n",
    "            already_seen_idx = np.where(np.isin(similarities[rep][dataset], already_seen))\n",
    "            user_recs    = np.delete(similarities[rep][dataset][user], already_seen_idx)\n",
    "            one_out_rank = np.where(user_recs==one_out)[0][0] + 1\n",
    "\n",
    "            metrics_1[rep][dataset]['hitrate@10'].append(int(one_out_rank <= K))\n",
    "            metrics_1[rep][dataset]['mmr@10'].append(0 if one_out_rank > K else 1/one_out_rank)\n",
    "            metrics_1[rep][dataset]['ndcg@10'].append(0 if one_out_rank > K else math.log(2)/math.log(1+one_out_rank))\n",
    "\n",
    "        metrics_1[rep][dataset]['hitrate@10'] = np.mean(metrics_1[rep][dataset]['hitrate@10'])\n",
    "        metrics_1[rep][dataset]['mmr@10'] = np.mean(metrics_1[rep][dataset]['mmr@10'])\n",
    "        metrics_1[rep][dataset]['ndcg@10'] = np.mean(metrics_1[rep][dataset]['ndcg@10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rep in metrics_1:\n",
    "    for dataset in metrics_1[rep]:\n",
    "        print(rep.upper(), '-', dataset) \n",
    "        for metric in metrics_1[rep][dataset]:\n",
    "            print('    ' + metric.upper() + ':', round(metrics_1[rep][dataset][metric], 4))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"combo = {}\n",
    "\n",
    "combo_sim = []\n",
    "for rep in [ 'transd']:\n",
    "    talk_features = representations[rep]\n",
    "    user_features = user_vectors[rep]\n",
    "    if len(combo_sim) == 0:\n",
    "        combo_sim = cosine_similarity(user_features, talk_features)\n",
    "    else:\n",
    "        combo_sim += cosine_similarity(user_features, talk_features)\n",
    "\n",
    "combo_1 = {user: combo_sim[user2idx[user]].argsort()[::-1] for user in test_dataset}\n",
    "\n",
    "rep = 'combo_2'\n",
    "metrics[rep] = {'hitrate@10': [], 'mmr@10': [], 'ndcg@10': []}\n",
    "\n",
    "for user in test_dataset:\n",
    "    # user_idx  = user2idx[user]\n",
    "    already_seen = [talk2idx[t] for t in train_dataset[user]]\n",
    "    already_seen_idx = np.where(np.isin(combo_1[user], already_seen))\n",
    "    user_recs = np.delete(combo_1[user], already_seen_idx)\n",
    "    one_out   = talk2idx[test_dataset[user][0]]\n",
    "    one_out_rank = np.where(user_recs==one_out)[0][0] + 1\n",
    "\n",
    "    metrics[rep]['hitrate@10'].append(int(one_out_rank <= K))\n",
    "    metrics[rep]['mmr@10'].append(0 if one_out_rank > K else 1/one_out_rank)\n",
    "    metrics[rep]['ndcg@10'].append(0 if one_out_rank > K else math.log(2)/math.log(1+one_out_rank))\n",
    "\n",
    "metrics[rep]['hitrate@10'] = np.mean(metrics[rep]['hitrate@10'])\n",
    "metrics[rep]['mmr@10'] = np.mean(metrics[rep]['mmr@10'])\n",
    "metrics[rep]['ndcg@10'] = np.mean(metrics[rep]['ndcg@10'])\n",
    "\n",
    "print(rep.upper())\n",
    "for metric in metrics[rep]:\n",
    "    print('    ' + metric.upper() + ':', round(metrics[rep][metric], 4))\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
