{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string \n",
    "import pickle\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix as sparse_matrix\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize  \n",
    "\n",
    "from gensim.test import utils\n",
    "from gensim.models import KeyedVectors, nmf\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sentence_transformers import models, SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_talks = pd.read_csv('../Data/talks_data.csv')\n",
    "df_users = pd.read_csv('../users_data.csv')\n",
    "\n",
    "df_train = pd.read_csv('../Data/TED_train.csv')\n",
    "df_test  = pd.read_csv('../Data/TED_test.csv')\n",
    "talks_ids = {k:i for i,k in pickle.load(open('../Data/dict_talks_idx.pickle', 'rb')).items()}\n",
    "users_ids = {k:i for i,k in pickle.load(open('../Data/dict_users_idx.pickle', 'rb')).items()}\n",
    "\n",
    "df_train['user'] = df_train['user_id'].apply(lambda u: users_ids[u])\n",
    "df_train['talk'] = df_train['talk_id'].apply(lambda u: talks_ids[u])\n",
    "df_test['user'] = df_test['user_id'].apply(lambda u: users_ids[u])\n",
    "df_test['talk'] = df_test['talk_id'].apply(lambda u: talks_ids[u])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_talks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.examples import sentences \n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {}\n",
    "for i, talk in tqdm(df_talks.iterrows(), total=len(df_talks)):\n",
    "    content = talk['title'] + ' ' + talk['description'] + ' ' + talk['transcript'] \n",
    "    talk_hash = talk['id']\n",
    "    entities[talk_hash] = []\n",
    "    doc = nlp(content)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        entities[talk_hash].append((ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['062dd0f773cd5999a09714a371e1f8017163e2a1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(entities, open('entities.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enamex_entities = {}\n",
    "all_types = set()\n",
    "all_entities = []\n",
    "for talk in entities:\n",
    "    enamex = set()\n",
    "    for entity, etype in entities[talk]:\n",
    "        all_types.add(etype)\n",
    "        if etype in ['PERSON', 'LOC', 'ORG', 'GPE', 'ORG', 'FAC', 'PRODUCT', 'WORK_OF_ART']:\n",
    "            all_entities.append(entity)\n",
    "            enamex.add((entity, etype))\n",
    "        enamex_entities[talk] = enamex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counter = Counter(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counter.most_common()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enamex_entities['062dd0f773cd5999a09714a371e1f8017163e2a1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../kg_embeddings/ted-ner-all.txt', 'w') as f:\n",
    "    for talk in enamex_entities:\n",
    "        for entity, etype in enamex_entities[talk]:\n",
    "            h = talk\n",
    "            r = 'mentions'\n",
    "            t = entity\n",
    "            if entity_counter[entity] > 1:\n",
    "                f.write(f'{h}\\t{r}\\t{t}\\n')\n",
    "            else:\n",
    "                print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../kg_embeddings/metadata-interactions-ner/ted-ner-min10.txt', 'w') as f:\n",
    "    for talk in enamex_entities:\n",
    "        for entity, etype in enamex_entities[talk]:\n",
    "            h = talk\n",
    "            r = 'mentions'\n",
    "            t = entity\n",
    "            if entity_counter[entity] > 9:\n",
    "                f.write(f'{h}\\t{r}\\t{t}\\n')\n",
    "            else:\n",
    "                print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
